# -*- coding: utf-8 -*-
"""metadata_samil.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nqLqfXQIikQBj6tIK5nAomahHx1ao4Vm
"""

from google.colab import drive

import warnings
warnings.filterwarnings('ignore')

# Commented out IPython magic to ensure Python compatibility.
import IPython.display as ipd
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
# %matplotlib inline
import seaborn as sns
import sklearn as skl
import sklearn.svm

drive.mount('/content/drive')

!cp /content/drive/MyDrive/project/utils.py /content/utils.py
!pip install python-dotenv
import utils

tracks = utils.load('/content/drive/MyDrive/project/fma_metadata/tracks.csv')
genres = utils.load('/content/drive/MyDrive/project/fma_metadata/genres.csv')
features = utils.load('/content/drive/MyDrive/project/fma_metadata/features.csv')
echonest = utils.load('/content/drive/MyDrive/project/fma_metadata/echonest.csv')

genres.shape

small = tracks['set', 'subset'] <= 'small'

train = tracks['set', 'split'] == 'training'
val = tracks['set', 'split'] == 'validation'
test = tracks['set', 'split'] == 'test'

y_train = tracks.loc[small & train, ('track', 'genre_top')]
y_test = tracks.loc[small & test, ('track', 'genre_top')]
y_val = tracks.loc[small & val, ('track', 'genre_top')]
X_train = features.loc[small & train, 'mfcc']
X_val = features.loc[small & val, 'mfcc']
X_test = features.loc[small & test, 'mfcc']

print('{} training examples, {} testing examples'.format(y_train.size, y_test.size))
print('{} features, {} classes'.format(X_train.shape[1], np.unique(y_train).size))

deleted = [99134, 108925, 133297, 98565, 98567, 98569]
X_train.drop(labels = deleted, axis = 0, inplace = True)
y_train.drop(labels = deleted, axis = 0, inplace = True)

X_TRAIN = echonest.loc[small & train, 'echonest']
X_TEST = echonest.loc[small & test, 'echonest']
X_VAL = echonest.loc[small & val, 'echonest']
X_TRAIN = X_TRAIN.loc[:, 'audio_features']
X_TEST = X_TEST.loc[:, 'audio_features']
X_VAL = X_VAL.loc[:, 'audio_features']
train_indexes = np.array(X_TRAIN.index)
test_indexes = np.array(X_TEST.index)
val_indexes = np.array(X_VAL.index)
echo_Y_train = y_train[train_indexes]
echo_Y_test = y_test[test_indexes]
echo_Y_val = y_val[val_indexes]

# np.unique(echonest.loc[small & train].keys())
# np.unique(features.loc[small & train].keys())#['mfcc']

def normalize(frame):
    min_val = frame.min()
    max_val = frame.max()
    frame = (frame - min_val) / (max_val - min_val)
    return frame

X_train_N = normalize(X_train)
X_test_N = normalize(X_test)
X_val_N = normalize(X_val)
X_TRAIN_N = normalize(X_TRAIN)
X_TEST_N = normalize(X_TEST)
X_VAL_N = normalize(X_VAL)

list(y_train.cat.categories)

dct = {
 'Blues':                0,
 'Classical':            1,
 'Country':              2,
 'Easy Listening':       3,
 'Electronic':           4, 
 'Experimental':         5,
 'Folk':                 6,
 'Hip-Hop':              7,
 'Instrumental':         8,
 'International':        9,
 'Jazz':                 10,
 'Old-Time / Historic':  11,
 'Pop':                  12,
 'Rock':                 13,
 'Soul-RnB':             14,
 'Spoken':               15
}
train_y = dict()
test_y = dict()
val_y = dict()
for i in y_train.keys():
    train_y[i] = dct[y_train[i]]
for i in y_test.keys():
    test_y[i] = dct[y_test[i]]
for i in y_val.keys():
    val_y[i] = dct[y_val[i]]

lin_reg_y_train = np.array(list(train_y.values()))
lin_reg_y_test = np.array(list(test_y.values()))
lin_reg_y_val = np.array(list(val_y.values()))

np.unique(y_train)

"""#TELEMETRIES"""

# Be sure training samples are shuffled.
X_train, y_train = skl.utils.shuffle(X_train, y_train, random_state=42)

# Standardize features by removing the mean and scaling to unit variance.
scaler = skl.preprocessing.StandardScaler(copy=False)
scaler.fit_transform(X_train)
scaler.transform(X_test)

def train_poly(tol, deg):
    clf = skl.svm.SVC(kernel = 'poly', tol = tol, degree = deg)
    clf.fit(X_train, y_train)
    score = clf.score(X_test, y_test)
    return score

def train_rbf(tol):
    clf = skl.svm.SVC(kernel = 'rbf', tol = tol)
    clf.fit(X_train, y_train)
    score = clf.score(X_test, y_test)
    return score

print('SVM Polynomial Kernel: ')
for i in (1, 2, 3, 4):
    for j in (1e0, 1e-1, 1e-2, 1e-3):
        print('degree = %d, tolerance = %f, accuracy = %.2f'%(i, j, 100*train_poly(i, j)))

print('\nSVM rbf Kernel: ')
for i in (1e0, 1e-1, 1e-2, 1e-3):
    print('tolerance = %f, accuracy = %.2f'%(i, 100*train_rbf(i)))

clf = skl.svm.SVC(kernel = 'rbf', tol = 0.01)
clf.fit(X_train, y_train)
clf.score(X_val, y_val)

from sklearn.neighbors import KNeighborsClassifier

for i in range(1,50):
    neigh = KNeighborsClassifier(n_neighbors=i*2+1)
    neigh.fit(X_train, y_train)
    score = neigh.score(X_test,y_test)
    print('kNN accuracy when k = ',i*2+1,': {:.2%}'.format(score))

neigh.score(X_val, y_val)

from sklearn.naive_bayes import GaussianNB
clf = GaussianNB()
clf.fit(X_train, y_train)
score = clf.score(X_test, y_test)
# score = clf.score(X_val, y_val)
print('Gaussian naive bayes accuracy: {:.2%}'.format(score))

from sklearn.naive_bayes import ComplementNB
clf = ComplementNB()
clf.fit(X_train_N, lin_reg_y_train)
score = clf.score(X_test_N, lin_reg_y_test)
# score = clf.score(X_val_N, lin_reg_y_val)
print('Categorical naive bayes accuracy: {:.2%}'.format(score))

from sklearn.naive_bayes import MultinomialNB
clf = MultinomialNB()
clf.fit(X_train_N, lin_reg_y_train)
score = clf.score(X_test_N, lin_reg_y_test)
# score = clf.score(X_val_N, lin_reg_y_val)
print('Multinomial naive bayes accuracy: {:.2%}'.format(score))

from sklearn.linear_model import LogisticRegression
# reg = LogisticRegression(multi_class='multinomial', max_iter = 100).fit(X_train_N, lin_reg_y_train)
# score = reg.score(X_test_N, lin_reg_y_test)
# print('Logistic regression accuracy: {:.2%}'.format(score))
# reg = LogisticRegression(multi_class='multinomial', max_iter = 175).fit(X_train_N, lin_reg_y_train)
# score = reg.score(X_test_N, lin_reg_y_test)
# print('Logistic regression accuracy: {:.2%}'.format(score))
# reg = LogisticRegression(multi_class='multinomial', max_iter = 200).fit(X_train_N, lin_reg_y_train)
# score = reg.score(X_test_N, lin_reg_y_test)
# print('Logistic regression accuracy: {:.2%}'.format(score))
# reg = LogisticRegression(multi_class='multinomial', max_iter = 225).fit(X_train_N, lin_reg_y_train)
# score = reg.score(X_test_N, lin_reg_y_test)
# print('Logistic regression accuracy: {:.2%}'.format(score))
reg = LogisticRegression(multi_class='multinomial', max_iter = 500).fit(X_train_N, lin_reg_y_train)
score = reg.score(X_test_N, lin_reg_y_test)
print('Logistic regression accuracy: {:.2%}'.format(score))
score = reg.score(X_val_N, lin_reg_y_val)
print('Logistic regression accuracy: {:.2%}'.format(score))

from sklearn.linear_model import Perceptron
reg = Perceptron().fit(X_train_N, lin_reg_y_train)
score = reg.score(X_test_N, lin_reg_y_test)
score = reg.score(X_val_N, lin_reg_y_val)
print('Perceptron accuracy: {:.2%}'.format(score))

"""#OYNAKLIK

"""

# Be sure training samples are shuffled.
X_TRAIN_N, echo_Y_train = skl.utils.shuffle(X_TRAIN_N, echo_Y_train, random_state=42)

# Standardize features by removing the mean and scaling to unit variance.
scaler = skl.preprocessing.StandardScaler(copy=False)
scaler.fit_transform(X_TRAIN_N)
scaler.transform(X_TEST_N)

def train_poly(tol, deg):
    clf = skl.svm.SVC(kernel = 'poly', tol = tol, degree = deg)
    clf.fit(X_TRAIN_N, echo_Y_train)
    score = clf.score(X_TEST_N, echo_Y_test)
    return score

def train_rbf(tol):
    clf = skl.svm.SVC(kernel = 'rbf', tol = tol)
    clf.fit(X_TRAIN_N, echo_Y_train)
    score = clf.score(X_TEST_N, echo_Y_test)
    return score

print('SVM Polynomial Kernel: ')
for i in (1, 2, 3, 4):
    for j in (1e0, 1e-1, 1e-2, 1e-3):
        print('degree = %d, tolerance = %f, accuracy = %.2f'%(i, j, 100*train_poly(i, j)))

print('\nSVM rbf Kernel: ')
for i in (1e0, 1e-1, 1e-2, 1e-3):
    print('tolerance = %f, accuracy = %.2f'%(i, 100*train_rbf(i)))

clf = skl.svm.SVC(kernel = 'rbf', tol = 0.1)
clf.fit(X_TRAIN_N, echo_Y_train)
clf.score(X_VAL_N, echo_Y_val)

from sklearn.neighbors import KNeighborsClassifier

for i in range(1,50):
    neigh = KNeighborsClassifier(n_neighbors=i*2+1)
    neigh.fit(X_TRAIN_N, echo_Y_train)
    score = neigh.score(X_TEST_N, echo_Y_test)
    print('kNN accuracy when k = ',i*2+1,': {:.2%}'.format(score))

neigh = KNeighborsClassifier(n_neighbors=21)
neigh.fit(X_TRAIN_N, echo_Y_train)
neigh.score(X_VAL_N, echo_Y_val)

from sklearn.naive_bayes import GaussianNB
clf = GaussianNB()
clf.fit(X_TRAIN_N, echo_Y_train)
score = clf.score(X_TEST_N, echo_Y_test)
# score = clf.score(X_VAL_N, echo_Y_val)
print('Gaussian naive bayes accuracy: {:.2%}'.format(score))

from sklearn.naive_bayes import MultinomialNB
clf = MultinomialNB()
clf.fit(X_TRAIN_N, echo_Y_train)
score = clf.score(X_TEST_N, echo_Y_test)
score = clf.score(X_VAL_N, echo_Y_val)
print('Multinomial naive bayes accuracy: {:.2%}'.format(score))

TRAIN_Y = dict()
TEST_Y = dict()
VAL_Y = dict()
for i in echo_Y_train.keys():
    TRAIN_Y[i] = dct[echo_Y_train[i]]
for i in echo_Y_test.keys():
    TEST_Y[i] = dct[echo_Y_test[i]]
for i in echo_Y_val.keys():
    VAL_Y[i] = dct[echo_Y_val[i]]
lin_reg_y_TRAIN = np.array(list(TRAIN_Y.values()))
lin_reg_y_TEST = np.array(list(TEST_Y.values()))
lin_reg_y_VAL = np.array(list(VAL_Y.values()))

from sklearn.naive_bayes import ComplementNB
clf = ComplementNB()
clf.fit(X_TRAIN_N, lin_reg_y_TRAIN)
score = clf.score(X_TEST_N, lin_reg_y_TEST)
# score = clf.score(X_VAL_N, lin_reg_y_VAL)
print('Categorical naive bayes accuracy: {:.2%}'.format(score))

from sklearn.linear_model import LogisticRegression
reg = LogisticRegression(multi_class='multinomial', max_iter = 10).fit(X_TRAIN_N, lin_reg_y_TRAIN)
score = reg.score(X_TEST_N, lin_reg_y_TEST)
print('10: Logistic regression accuracy: {:.2%}'.format(score))
reg = LogisticRegression(multi_class='multinomial', max_iter = 20).fit(X_TRAIN_N, lin_reg_y_TRAIN)
score = reg.score(X_TEST_N, lin_reg_y_TEST)
print('20: Logistic regression accuracy: {:.2%}'.format(score))
reg = LogisticRegression(multi_class='multinomial', max_iter = 50).fit(X_TRAIN_N, lin_reg_y_TRAIN)
score = reg.score(X_TEST_N, lin_reg_y_TEST)
print('50: Logistic regression accuracy: {:.2%}'.format(score))
score = reg.score(X_VAL_N, lin_reg_y_VAL)
print('Validation Logistic regression accuracy: {:.2%}'.format(score))
reg = LogisticRegression(multi_class='multinomial', max_iter = 75).fit(X_TRAIN_N, lin_reg_y_TRAIN)
score = reg.score(X_TEST_N, lin_reg_y_TEST)
print('75: Logistic regression accuracy: {:.2%}'.format(score))
reg = LogisticRegression(multi_class='multinomial', max_iter = 100).fit(X_TRAIN_N, lin_reg_y_TRAIN)
score = reg.score(X_TEST_N, lin_reg_y_TEST)
print('100: Logistic regression accuracy: {:.2%}'.format(score))

train_indexes.shape

X_TRAIN

X_TRAIN

X_TEST_N
echo_Y_test

tracks

a = []
for i in features.keys():
    a.append(i[1])
np.unique(a)

train_indexes.shape

features.keys()

